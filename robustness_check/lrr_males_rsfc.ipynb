{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fa9da7-a558-4ca1-b84e-17b13f115274",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy pandas matplotlib seaborn statsmodels scipy scikit-learn\n",
    "\n",
    "#import relevant libraries\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys; sys.path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statsmodels.stats.multitest import multipletests as fdr\n",
    "from matplotlib import colors\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.metrics import explained_variance_score, r2_score, classification_report\n",
    "from sklearn.linear_model import Ridge, RidgeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, GroupKFold, GroupShuffleSplit, StratifiedKFold\n",
    "from sklearn.svm import SVC\n",
    "from scipy import stats\n",
    "from sklearn.utils import shuffle\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3830631d-5006-40ce-a087-61b2e6380503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with rsfc predictor data\n",
    "ABCD_base_dir = 'PATH_TO_DIR'\n",
    "ABCD_results_dir = 'PATH_TO_DIR'\n",
    "\n",
    "data = pd.read_csv(ABCD_base_dir, 'males_predictors.csv'), header=None)\n",
    "fc_m = pd.DataFrame(data)\n",
    "\n",
    "# Access the values as a numpy array\n",
    "values = fc_m.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c29da023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with labels\n",
    "data = pd.read_csv(ABCD_base_dir, 'males_labels.csv'), header=None)\n",
    "gender_p_m_sum = pd.DataFrame(data)\n",
    "\n",
    "# Access the values as a numpy array\n",
    "T = gender_p_m_sum.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc184b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame with site data\n",
    "data = pd.read_csv(ABCD_base_dir, 'males_site.csv'), header=None)\n",
    "site_m = pd.DataFrame(data)\n",
    "\n",
    "# Access the values as a numpy array\n",
    "values = site_m.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312125a6-e2c8-4aaa-8d0a-11a204ce733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up predictive models\n",
    "#number of repetitions you want to perform (100 for 'true', 1000 for 'null')\n",
    "rep = 100\n",
    "\n",
    "#number of folds you want in the cross-validation\n",
    "k = 3\n",
    "#proportion of data you want in your training set and test set\n",
    "train_size = .80\n",
    "test_size = 1-train_size\n",
    "\n",
    "#regression model type\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline with standardization and Ridge regression\n",
    "regr = make_pipeline(StandardScaler(), Ridge(max_iter=1000000, solver='lsqr'))\n",
    "\n",
    "#set hyperparameter grid space you want to search through for the model\n",
    "#adapted from the Thomas Yeo Lab Github: \n",
    "#ThomasYeoLab/CBIG/blob/master/stable_projects/predict_phenotypes/He2019_KRDNN/KR_HCP/CBIG_KRDNN_KRR_HCP.m\n",
    "#alphas = [0, 0.00001, 0.0001, 0.001, 0.004, 0.007, 0.01, 0.04, 0.07, 0.1, 0.4, 0.7, 1, 1.5, 2, 2.5, 3,\n",
    "#          3.5, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100, 150, 200, 300, 500, 700, 1000, 2000, \n",
    "#          3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 2, 3, 4, 5, 10, 15, 20, 30, 40, 50, 60, 70, 80, 100, \n",
    "          150, 200, 300, 500, 700, 1000]\n",
    "\n",
    "#alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "\n",
    "#param grid set to the hyperparamters you want to search through\n",
    "paramGrid ={'ridge__alpha': alphas}\n",
    "\n",
    "#set x data to be the input variable you want to use\n",
    "X = fc_m.values\n",
    "Y = np.ravel(gender_p_m_sum.T)\n",
    "site = np.asarray(site_m.values).astype(int)\n",
    "site = site.ravel()\n",
    "\n",
    "#number of features \n",
    "n_feat = X.shape[1]\n",
    "\n",
    "pred_name = 'youth_report'\n",
    "pred_sex = 'm'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fa28fb-5f35-466e-9a32-2e26ea553555",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create variables to store relevant data/outputs\n",
    "#r^2 - coefficient of determination\n",
    "r2 = np.zeros([rep])\n",
    "#explained variance\n",
    "var = np.zeros([rep])\n",
    "#correlation between true and predicted (aka prediction accuracy)\n",
    "corr = np.zeros([rep])\n",
    "#optimised alpha (hyperparameter)\n",
    "opt_alpha = np.zeros([rep])\n",
    "#predictions made by the model\n",
    "#don't need to save any of these right now\n",
    "#preds = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#true test values for cognition\n",
    "#cogtest = np.zeros([rep,n_cog,int(np.ceil(X.shape[0]*test_size))])\n",
    "#feature importance extracted from the model\n",
    "featimp = np.zeros([rep,n_feat])\n",
    "#for when the feat weights get haufe-inverted\n",
    "#featimp_haufe = np.zeros([rep,n_feat])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1e4cf9-5062-422b-bf05-14cc2d1e5695",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#iterate through the diff train/test splits\n",
    "\n",
    "#alphas = np.load(ABCD_results_dir + '/fc_alpha_' + pred_name + '_opt.npy') ##only need this for the null ones\n",
    "\n",
    "for p in range(rep):\n",
    "    \n",
    "    #print model # you're on\n",
    "    print('Model %d' %(p+1))\n",
    "    \n",
    "    #Y_shuffle = shuffle(Y, random_state=p) ##only need this for the null ones\n",
    "    \n",
    "    #print time\n",
    "    now = datetime.now()\n",
    "    current_time = now.strftime(\"%H:%M:%S\")\n",
    "    print(\"Current Time =\", current_time)\n",
    "    \n",
    "    #split into train/test data\n",
    "    train_inds, test_inds = next(GroupShuffleSplit(test_size=1-train_size, n_splits=1, random_state = p).split(X, groups=site))\n",
    "    \n",
    "    #set x values based on indices from split\n",
    "    x_train = X[train_inds]\n",
    "    x_test = X[test_inds]\n",
    "        \n",
    "    #set y values based on indices from split  \n",
    "    beh_train = Y[train_inds]\n",
    "    beh_test = Y[test_inds]\n",
    "    \n",
    "    #set y values based on indices from split ##only need this for the null ones\n",
    "    #beh_train = Y_shuffle[train_inds]\n",
    "    #beh_test = Y_shuffle[test_inds]\n",
    "    \n",
    "    site_train = site[train_inds]\n",
    "    site_test = site[test_inds] \n",
    "    \n",
    "    #convert y values to to double\n",
    "    y_train = np.double(beh_train)\n",
    "    y_test = np.double(beh_test)\n",
    "\n",
    "\n",
    "\n",
    "    #create variables to store nested CV scores, and best parameters from hyperparameter optimisation\n",
    "    best_scores = []\n",
    "    best_params = []\n",
    "    \n",
    "\n",
    "        \n",
    "    #set parameters for inner and outer loops for CV\n",
    "    cv_split = GroupKFold(n_splits=k)\n",
    "        \n",
    "    #print (\"Optimising Models\")\n",
    "            \n",
    "    #define regressor with grid-search CV for inner loop\n",
    "    gridSearch = GridSearchCV(estimator=regr, param_grid=paramGrid, n_jobs=-1, verbose=0, cv=cv_split, scoring='explained_variance')\n",
    "    \n",
    "    #fit regressor to the model, use site ID as group category again\n",
    "    gridSearch.fit(x_train, y_train, groups=site_train)\n",
    "\n",
    "    #save parameters corresponding to the best score\n",
    "    best_params.append(list(gridSearch.best_params_.values()))\n",
    "    best_scores.append(gridSearch.best_score_)\n",
    "        \n",
    "\n",
    "        \n",
    "    #print (\"Evaluating Models\")\n",
    "        \n",
    "    #save optimised alpha values\n",
    "    opt_alpha[p] = best_params[best_scores.index(np.max(best_scores))][0]\n",
    "    \n",
    "    \n",
    "    #rand_alpha = np.random.choice(alphas)\n",
    "    \n",
    "    #fit optimized models\n",
    "    #model = Ridge(alpha = opt_alpha[p], normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "    model = make_pipeline(StandardScaler(), Ridge(alpha = opt_alpha[p], max_iter=1000000, solver='lsqr'))\n",
    "\n",
    "    #model = Ridge(alpha = rand_alpha, normalize=True, max_iter=1000000, solver='lsqr')\n",
    "\n",
    "    model.fit(x_train, y_train);\n",
    "        \n",
    "        \n",
    "    #evaluate model\n",
    "    r2[p]=model.score(x_test,y_test)\n",
    "    #print(r2[p])\n",
    "        \n",
    "    #generate predictions in test set\n",
    "    preds = []\n",
    "    preds = model.predict(x_test).ravel()\n",
    "    \n",
    "        \n",
    "    #compute explained variance \n",
    "    var[p] = explained_variance_score(y_test, preds)\n",
    "    #print(var[p])\n",
    "\n",
    "\n",
    "    #compute correlation between true and predicted (prediction accuracy)\n",
    "    corr[p] = np.corrcoef(y_test.ravel(), preds)[1,0]\n",
    "    #print(corr[p])\n",
    "    \n",
    "    #print (\"Haufe-Transforming Feature Weights\")\n",
    "    cov_x = []\n",
    "    cov_y = []\n",
    "    \n",
    "    #extract feature importance\n",
    "    featimp[p,:] = model.named_steps['ridge'].coef_\n",
    "    #compute Haufe-inverted feature weights\n",
    "    cov_x = np.cov(np.transpose(x_train))\n",
    "    cov_y = np.cov(y_train)\n",
    "    #featimp_haufe[p,:] = np.matmul(cov_x,featimp[p,:])*(1/cov_y)\n",
    "                \n",
    "    #save results\n",
    "    np.save((ABCD_results_dir + '/fc_r2_' + pred_name + pred_sex + '.npy'),r2)\n",
    "    np.save((ABCD_results_dir + '/fc_var_' + pred_name + pred_sex + '.npy'),var)\n",
    "    np.save((ABCD_results_dir + '/fc_corr_' + pred_name + pred_sex + '.npy'),corr)\n",
    "    #np.save((ABCD_results_dir + '/fc_alpha_' + pred_name + pred_sex + '.npy'),opt_alpha)\n",
    "    #np.save((ABCD_results_dir + '/fc_featimp_' + pred_name + pred_sex + '.npy'),featimp)\n",
    "    #np.save((ABCD_results_dir + '/fc_featimp_haufe_' + pred_name + pred_sex + '.npy'),featimp_haufe_f)\n",
    "\n",
    "print(\"Finished\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
